# PHASE 4: Integration & Deployment - Detailed Plan

**Date**: October 22, 2025  
**Duration**: 3-4 days (est.)  
**Status**: ðŸŸ¢ STARTING  
**Owner**: fulgidus (Backend Lead)  

---

## Overview

Phase 4 focuses on:
- âœ… End-to-End integration testing (all services working together)
- âœ… Docker Compose deployment validation
- âœ… Performance benchmarking and load testing
- âœ… Cross-service integration (API Gateway, Training, Data Ingestion)
- âœ… Monitoring and alerting setup
- âœ… Production readiness validation

**Handoff from Phase 3**: RF Acquisition service is complete and tested (89% coverage, all critical paths passing).

---

## Part A: Integration & Testing (Days 1-2)

### Task A1: E2E Test Suite (Priority: **HIGH**, Time: 4 hours)

**Objective**: Validate that all services work together end-to-end.

**Files to create**:
- `services/rf-acquisition/tests/e2e/conftest.py` - Shared fixtures
- `services/rf-acquisition/tests/e2e/test_complete_workflow.py` - Main E2E tests

**What we're testing**:
```
POST /api/v1/acquisition/acquire
  â†“
Celery task triggered (acquire_iq)
  â”œâ”€ Fetches IQ from 7 WebSDRs (concurrent)
  â”œâ”€ Processes metrics (SNR, frequency offset)
  â”œâ”€ Saves to MinIO (s3://heimdall-raw-iq/sessions/{task_id}/)
  â”œâ”€ Saves to TimescaleDB (measurements table)
  â””â”€ Returns task_id
  â†“
GET /api/v1/acquisition/status/{task_id}
  â”œâ”€ Returns state (PENDING, SUCCESS, FAILED)
  â”œâ”€ Returns progress %
  â””â”€ Returns results when done
  â†“
Verify data:
  â”œâ”€ MinIO has .npy and .json files
  â”œâ”€ Database has 7 measurement records
  â””â”€ Metrics are correct (SNR > 0, etc.)
```

**Test cases** (in `test_complete_workflow.py`):

```python
# 1. Happy path - successful acquisition
def test_acquisition_complete_workflow():
    # Trigger acquisition
    response = post("/api/v1/acquisition/acquire", {
        "frequency_mhz": 145.50,
        "duration_seconds": 5.0
    })
    assert response.status_code == 202
    task_id = response.json()["task_id"]
    
    # Poll until complete
    for attempt in range(30):
        status = get(f"/api/v1/acquisition/status/{task_id}")
        state = status.json()["state"]
        if state == "SUCCESS":
            break
        sleep(1)
    
    # Verify results
    assert state == "SUCCESS"
    
    # Check MinIO
    minio_files = minio_client.list_objects(f"sessions/{task_id}/")
    assert len(minio_files) >= 7  # At least 7 WebSDRs
    
    # Check Database
    measurements = db.query(f"SELECT * FROM measurements WHERE task_id = '{task_id}'")
    assert len(measurements) == 7
    assert all(m.snr > 0 for m in measurements)

# 2. Partial failure - one WebSDR offline
def test_websdr_partial_failure():
    # Mock one WebSDR as offline
    with patch("src.websdr_fetcher.fetch_websdr") as mock:
        mock.side_effect = [
            websdr_data[0:6],  # First 6 work
            Exception("Connection timeout"),  # 7th fails
        ]
        
        response = post("/api/v1/acquisition/acquire", {...})
        task_id = response.json()["task_id"]
        
        wait_for_completion(task_id)
        status = get(f"/api/v1/acquisition/status/{task_id}")
        
        # Should still succeed (with 6/7 WebSDRs)
        assert status.json()["state"] == "SUCCESS"
        assert len(status.json()["errors"]) == 1
        
        # Database should have 6 measurements
        measurements = db.query(f"SELECT * FROM measurements WHERE task_id = '{task_id}'")
        assert len(measurements) == 6

# 3. Concurrent acquisitions
def test_concurrent_acquisitions():
    task_ids = []
    
    # Trigger 5 concurrent acquisitions
    for i in range(5):
        response = post("/api/v1/acquisition/acquire", {
            "frequency_mhz": 145.50 + i*0.1,
            "duration_seconds": 5.0
        })
        task_ids.append(response.json()["task_id"])
    
    # Wait for all to complete
    for task_id in task_ids:
        wait_for_completion(task_id)
        status = get(f"/api/v1/acquisition/status/{task_id}")
        assert status.json()["state"] == "SUCCESS"
    
    # Verify no race conditions (all tasks independent)
    for task_id in task_ids:
        measurements = db.query(f"SELECT * FROM measurements WHERE task_id = '{task_id}'")
        assert len(measurements) == 7
```

**Validation criteria**:
- [ ] All test cases pass
- [ ] No race conditions in database
- [ ] MinIO data is correctly stored
- [ ] Task completion time < 30 seconds (for 5-second acquisition)
- [ ] Partial failures handled gracefully

---

### Task A2: Docker Integration Validation (Priority: **HIGH**, Time: 2 hours)

**Objective**: Verify all containers are running correctly.

**Steps**:
```bash
# 1. Check all containers running
docker compose ps

# 2. Verify health checks
docker compose ps --format "table {{.Service}}\t{{.Status}}"

# 3. Check logs for errors
docker compose logs --no-log-prefix | grep -i error

# 4. Test API endpoints
curl http://localhost:8000/health    # API Gateway
curl http://localhost:8001/health    # RF Acquisition
curl http://localhost:8002/health    # Training
curl http://localhost:8003/health    # Inference
curl http://localhost:8004/health    # Data Ingestion Web

# 5. Run services inside container
docker compose exec rf-acquisition pytest tests/ -v --tb=short

# 6. Check database connectivity
docker compose exec rf-acquisition python -c "
import psycopg2
conn = psycopg2.connect('postgresql://heimdall_user:changeme@postgres:5432/heimdall')
print('âœ“ Database connection OK')
"
```

**Validation criteria**:
- [ ] All containers UP (not crashed/exited)
- [ ] Health checks passing
- [ ] No error messages in logs
- [ ] All API endpoints respond (200 or valid error)
- [ ] Services can connect to dependencies

---

### Task A3: Database Schema Verification (Priority: **MEDIUM**, Time: 1 hour)

**Objective**: Confirm database migrations are applied and schema is correct.

**SQL queries to run**:
```sql
-- Check tables exist
SELECT tablename FROM pg_tables WHERE schemaname='public';

-- Expected tables:
-- - websdr_stations
-- - measurements
-- - known_sources
-- - training_sessions
-- - models

-- Check measurements hypertable (TimescaleDB)
SELECT * FROM timescaledb_information.hypertables;

-- Expected: measurements table with time_bucket_gapfill support

-- Check constraints
\d measurements

-- Expected columns:
-- - id (uuid)
-- - task_id (uuid)
-- - websdr_id (text)
-- - frequency_mhz (float)
-- - snr (float)
-- - frequency_offset (float)
-- - timestamp (timestamptz)
-- - data_url (text)
```

**Validation criteria**:
- [ ] All required tables exist
- [ ] measurements table is a hypertable
- [ ] All columns present and typed correctly
- [ ] Indexes present for query performance

---

## Part B: Performance & Hardening (Days 2-3)

### Task B1: Load Testing (Priority: **MEDIUM**, Time: 3 hours)

**Objective**: Establish performance baseline and identify bottlenecks.

**Tools**: `pytest-benchmark` (built-in) or `locust` (external)

**Benchmarks to run**:

```python
# File: services/rf-acquisition/tests/performance/test_benchmarks.py

import pytest
from httpclient import AsyncClient

client = AsyncClient(base_url="http://localhost:8001")

# Benchmark 1: Single acquisition end-to-end
@pytest.mark.benchmark
def test_single_acquisition_latency(benchmark):
    def acquire_and_wait():
        response = client.post("/api/v1/acquisition/acquire", json={
            "frequency_mhz": 145.50,
            "duration_seconds": 2.0
        })
        task_id = response.json()["task_id"]
        
        # Poll until complete (max 30 seconds)
        for _ in range(30):
            status = client.get(f"/api/v1/acquisition/status/{task_id}")
            if status.json()["state"] == "SUCCESS":
                return status
            time.sleep(1)
    
    result = benchmark(acquire_and_wait)
    # Should complete in < 5 seconds (including wait)
    assert result.mean < 5.0

# Benchmark 2: Concurrent acquisitions
@pytest.mark.benchmark
def test_concurrent_acquisitions(benchmark):
    def concurrent_acquire(n=5):
        task_ids = []
        for i in range(n):
            response = client.post("/api/v1/acquisition/acquire", json={
                "frequency_mhz": 145.50 + i*0.1,
                "duration_seconds": 2.0
            })
            task_ids.append(response.json()["task_id"])
        
        # Wait for all
        for task_id in task_ids:
            for _ in range(30):
                status = client.get(f"/api/v1/acquisition/status/{task_id}")
                if status.json()["state"] == "SUCCESS":
                    break
                time.sleep(1)
        return task_ids
    
    result = benchmark(concurrent_acquire)
    # 5 concurrent should complete in < 10 seconds
    assert result.mean < 10.0

# Benchmark 3: API response time (without waiting for completion)
@pytest.mark.benchmark
def test_api_trigger_latency(benchmark):
    def trigger():
        return client.post("/api/v1/acquisition/acquire", json={
            "frequency_mhz": 145.50,
            "duration_seconds": 2.0
        })
    
    result = benchmark(trigger)
    # API should respond in < 100ms
    assert result.mean < 0.1
```

**Performance targets**:
- API trigger latency: < 100ms
- Single acquisition (with wait): < 5 seconds
- 5 concurrent acquisitions: < 10 seconds
- Database write latency: < 500ms

---

### Task B2: Reliability Testing (Priority: **MEDIUM**, Time: 2 hours)

**Objective**: Verify graceful degradation when services are unavailable.

**Test scenarios**:

```python
# Scenario 1: Database offline
def test_database_offline():
    # Pause postgres container
    docker_compose("pause postgres")
    
    response = client.post("/api/v1/acquisition/acquire", json={...})
    assert response.status_code == 503  # Service Unavailable
    
    # Resume
    docker_compose("unpause postgres")

# Scenario 2: MinIO offline
def test_minio_offline():
    # Pause minio container
    docker_compose("pause minio")
    
    response = client.post("/api/v1/acquisition/acquire", json={...})
    assert response.status_code == 202  # Still triggers (queued)
    
    # Wait - should fail gracefully
    task_id = response.json()["task_id"]
    wait_for_failure(task_id)
    
    status = client.get(f"/api/v1/acquisition/status/{task_id}")
    assert status.json()["state"] == "FAILED"
    assert "MinIO" in status.json()["error_message"]
    
    # Resume
    docker_compose("unpause minio")

# Scenario 3: WebSDR offline
def test_websdr_offline():
    # Mock one WebSDR as unreachable
    # Should continue with other 6
    response = client.post("/api/v1/acquisition/acquire", json={...})
    task_id = response.json()["task_id"]
    
    wait_for_completion(task_id)
    status = client.get(f"/api/v1/acquisition/status/{task_id}")
    
    # Should succeed with partial data
    assert status.json()["state"] == "SUCCESS"
    assert len(status.json()["errors"]) > 0
    
    # Database should have 6/7 measurements
    measurements = db.query(f"SELECT * FROM measurements WHERE task_id = '{task_id}'")
    assert len(measurements) == 6

# Scenario 4: Task cancellation
def test_task_cancellation():
    response = client.post("/api/v1/acquisition/acquire", json={
        "frequency_mhz": 145.50,
        "duration_seconds": 30.0  # Long acquisition
    })
    task_id = response.json()["task_id"]
    
    # Wait a bit, then cancel
    time.sleep(2)
    cancel_response = client.post(f"/api/v1/acquisition/cancel/{task_id}")
    assert cancel_response.status_code == 200
    
    # Check status
    status = client.get(f"/api/v1/acquisition/status/{task_id}")
    assert status.json()["state"] == "CANCELLED"
```

**Validation criteria**:
- [ ] Database offline: API returns 503
- [ ] MinIO offline: Acquisition fails gracefully
- [ ] WebSDR offline: Continues with other receivers
- [ ] Task cancellation: Stops and cleans up resources

---

### Task B3: Monitoring Setup (Priority: **LOW**, Time: 2 hours)

**Objective**: Add Prometheus metrics and Grafana dashboards.

**Metrics to expose** (in `services/rf-acquisition/src/monitoring.py`):

```python
from prometheus_client import Counter, Histogram, Gauge

# Counters
acquisitions_total = Counter(
    'acquisitions_total',
    'Total number of acquisitions triggered'
)
acquisitions_succeeded = Counter(
    'acquisitions_succeeded_total',
    'Total successful acquisitions'
)
acquisitions_failed = Counter(
    'acquisitions_failed_total',
    'Total failed acquisitions'
)
websdr_failures = Counter(
    'websdr_failures_total',
    'Total WebSDR connection failures',
    ['websdr_id']
)

# Histograms
acquisition_duration = Histogram(
    'acquisition_seconds',
    'Acquisition duration in seconds',
    buckets=(1, 2, 5, 10, 20, 30)
)
iq_download_duration = Histogram(
    'iq_download_seconds',
    'IQ data download duration',
    buckets=(0.1, 0.5, 1, 2, 5)
)

# Gauges
active_acquisitions = Gauge(
    'active_acquisitions',
    'Number of currently active acquisitions'
)
celery_queue_depth = Gauge(
    'celery_queue_depth',
    'Number of tasks in Celery queue'
)
```

**Update `db/prometheus.yml`**:
```yaml
scrape_configs:
  - job_name: 'rf-acquisition'
    static_configs:
      - targets: ['rf-acquisition:8001']
    metrics_path: '/metrics'
```

**Grafana dashboard**:
- Acquisition success rate (%)
- Average acquisition duration
- WebSDR failure rate by receiver
- Celery queue depth
- Active acquisitions

---

## Part C: Cross-Service Integration (Days 3-4)

### Task C1: API Gateway Integration (Priority: **HIGH**, Time: 2 hours)

**Objective**: API Gateway proxies to RF Acquisition service.

**File**: `services/api-gateway/src/routers/acquisition.py`

```python
from fastapi import APIRouter, HTTPException
from httpx import AsyncClient
from typing import Optional

router = APIRouter(prefix="/acquisition", tags=["acquisition"])

RF_ACQUISITION_URL = "http://rf-acquisition:8001"

@router.post("/acquire")
async def trigger_acquisition(
    frequency_mhz: float,
    duration_seconds: float,
    description: Optional[str] = None
):
    """Trigger a new RF acquisition."""
    async with AsyncClient() as client:
        try:
            response = await client.post(
                f"{RF_ACQUISITION_URL}/api/v1/acquisition/acquire",
                json={
                    "frequency_mhz": frequency_mhz,
                    "duration_seconds": duration_seconds,
                    "description": description
                },
                timeout=5.0
            )
            return response.json()
        except Exception as e:
            raise HTTPException(status_code=503, detail=f"RF service unavailable: {e}")

@router.get("/status/{task_id}")
async def get_acquisition_status(task_id: str):
    """Get status of an acquisition task."""
    async with AsyncClient() as client:
        try:
            response = await client.get(
                f"{RF_ACQUISITION_URL}/api/v1/acquisition/status/{task_id}",
                timeout=5.0
            )
            return response.json()
        except Exception as e:
            raise HTTPException(status_code=503, detail=f"RF service unavailable: {e}")

@router.get("/measurements/{task_id}")
async def get_acquisition_measurements(task_id: str):
    """Get measurements from completed acquisition."""
    async with AsyncClient() as client:
        try:
            response = await client.get(
                f"{RF_ACQUISITION_URL}/api/v1/acquisition/measurements/{task_id}",
                timeout=10.0
            )
            return response.json()
        except Exception as e:
            raise HTTPException(status_code=503, detail=f"RF service unavailable: {e}")
```

**Update `services/api-gateway/src/main.py`**:
```python
from routers import acquisition

app = FastAPI()
app.include_router(acquisition.router)

@app.get("/health")
async def health():
    return {"status": "ok"}
```

**Validation criteria**:
- [ ] API Gateway proxy endpoints work
- [ ] Responses are correctly forwarded
- [ ] Error handling graceful (503 if RF service down)
- [ ] OpenAPI docs include acquisition endpoints

---

### Task C2: Training Service Integration (Priority: **MEDIUM**, Time: 2 hours)

**Objective**: Training service can query RF-captured data.

**File**: `services/training/src/data_loaders.py`

```python
from sqlalchemy import create_engine, select
from datetime import datetime, timedelta

class RFDataLoader:
    def __init__(self, database_url: str):
        self.engine = create_engine(database_url)
    
    def get_measurements_for_training(
        self,
        frequency_mhz: float,
        hours_back: int = 24
    ) -> list:
        """Fetch measurements for training data."""
        with self.engine.connect() as conn:
            cutoff_time = datetime.utcnow() - timedelta(hours=hours_back)
            
            query = f"""
            SELECT id, websdr_id, frequency_mhz, snr, frequency_offset, 
                   timestamp, data_url
            FROM measurements
            WHERE frequency_mhz = {frequency_mhz}
              AND timestamp > '{cutoff_time}'
            ORDER BY timestamp DESC
            """
            
            result = conn.execute(query)
            return [dict(row) for row in result]
    
    def get_measurements_by_session(self, task_id: str) -> list:
        """Fetch all measurements from a specific acquisition session."""
        with self.engine.connect() as conn:
            query = f"""
            SELECT * FROM measurements
            WHERE task_id = '{task_id}'
            """
            result = conn.execute(query)
            return [dict(row) for row in result]
```

**Validation criteria**:
- [ ] Training can query RF measurements
- [ ] Data is correctly loaded from database
- [ ] Performance acceptable (< 1 second per query)

---

### Task C3: Data Ingestion Web Integration (Priority: **MEDIUM**, Time: 2 hours)

**Objective**: Data Ingestion service can view/approve RF acquisitions.

**File**: `services/data-ingestion-web/src/routers/sessions.py`

```python
from fastapi import APIRouter
from httpx import AsyncClient
from pydantic import BaseModel
from datetime import datetime

router = APIRouter(prefix="/sessions", tags=["sessions"])

class RecordingSession(BaseModel):
    task_id: str
    frequency_mhz: float
    duration_seconds: float
    status: str
    websdr_count: int
    timestamp: datetime

@router.get("/recent")
async def get_recent_sessions():
    """Get recent RF acquisition sessions."""
    async with AsyncClient() as client:
        response = await client.get(
            "http://rf-acquisition:8001/api/v1/acquisition/sessions",
            timeout=5.0
        )
        return response.json()

@router.post("/approve/{task_id}")
async def approve_session(task_id: str):
    """Approve a session for training."""
    # Mark in database as approved
    db.execute(f"""
    UPDATE measurements SET approved=true 
    WHERE task_id = '{task_id}'
    """)
    return {"status": "approved"}

@router.post("/reject/{task_id}")
async def reject_session(task_id: str):
    """Reject a session."""
    db.execute(f"""
    UPDATE measurements SET approved=false 
    WHERE task_id = '{task_id}'
    """)
    return {"status": "rejected"}
```

**Validation criteria**:
- [ ] Data Ingestion can list RF sessions
- [ ] Can mark sessions as approved/rejected
- [ ] Approved data feeds into training pipeline

---

## Testing Roadmap

### Week 1 (Phase 4a: Integration)
- [x] Docker Compose all services running
- [ ] E2E test suite passing (Task A1)
- [ ] Database schema verified (Task A3)
- [ ] API Gateway proxying works (Task C1)

### Week 2 (Phase 4b: Performance)
- [ ] Load tests establish baselines (Task B1)
- [ ] Reliability tests all scenarios (Task B2)
- [ ] Monitoring metrics exposed (Task B3)

### Week 3 (Phase 4c: Cross-service)
- [ ] Training service integration (Task C2)
- [ ] Data Ingestion integration (Task C3)
- [ ] End-to-end workflow: RF â†’ Training â†’ Inference

---

## Success Criteria for Phase 4

âœ… **Functional**:
- All E2E tests passing
- Docker Compose deployment stable (no crashes)
- All services healthcheck passing
- API Gateway proxying RF acquisition endpoints

âœ… **Performance**:
- Single acquisition: < 5 seconds
- Concurrent acquisitions: < 10 seconds
- API latency: < 100ms

âœ… **Reliability**:
- Graceful handling of service failures
- Partial data collection when some WebSDRs offline
- Task cancellation working
- No data loss on errors

âœ… **Monitoring**:
- Prometheus metrics exposed
- Grafana dashboards showing key metrics
- Alert rules defined

---

## Next Phase (Phase 5)

- [ ] Advanced ML model training (PyTorch Lightning)
- [ ] Model versioning and registry (MLflow)
- [ ] Inference optimization (ONNX)
- [ ] High availability and scaling

---

## Commands Reference

```bash
# Run E2E tests
cd services/rf-acquisition
pytest tests/e2e/ -v --tb=short

# Run performance benchmarks
pytest tests/performance/ -v --benchmark-only

# Check all services
docker compose ps

# View logs
docker compose logs -f rf-acquisition

# Run inside container
docker compose exec rf-acquisition bash

# Stop and restart
docker compose restart rf-acquisition
```

---

**Phase 4 Status**: ðŸŸ¢ READY TO START  
**Estimated completion**: October 25, 2025
