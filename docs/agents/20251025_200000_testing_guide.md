# Testing Guide for Heimdall SDR

**Date**: 2025-10-25  
**Phase**: Comprehensive Testing Framework  
**Status**: Active

This document provides comprehensive guidance on writing, running, and maintaining tests for the Heimdall SDR project.

---

## Table of Contents

1. [Overview](#overview)
2. [Test Structure](#test-structure)
3. [Writing Tests](#writing-tests)
4. [Fixtures and Mocking](#fixtures-and-mocking)
5. [Running Tests](#running-tests)
6. [Coverage Requirements](#coverage-requirements)
7. [CI/CD Integration](#cicd-integration)
8. [Troubleshooting](#troubleshooting)

---

## Overview

Heimdall uses **pytest** as its testing framework with the following test categories:

- **Unit Tests**: Test individual functions/classes in isolation
- **Integration Tests**: Test service-to-service communication and infrastructure
- **E2E Tests**: Test complete workflows end-to-end

### Coverage Goals

- **Minimum**: 80% coverage per service
- **Target**: 90% coverage per service
- **Current**: See `coverage-report.json` for latest metrics

---

## Test Structure

### Directory Layout

```
services/
├── common/
│   └── test_fixtures.py        # Shared fixtures
├── rf-acquisition/
│   ├── src/                     # Source code
│   └── tests/
│       ├── unit/                # Unit tests
│       ├── integration/         # Integration tests
│       ├── e2e/                 # E2E tests
│       ├── mock_websdrs.py      # Mock WebSDR utilities
│       ├── fixtures.py          # Service-specific fixtures
│       └── conftest.py          # Pytest configuration
└── tests/                       # Cross-service tests
    ├── test_e2e_acquisition.py
    └── test_integration_services.py
```

### Test Markers

Use pytest markers to categorize tests:

```python
@pytest.mark.unit
def test_something():
    pass

@pytest.mark.integration
def test_service_communication():
    pass

@pytest.mark.e2e
def test_complete_workflow():
    pass

@pytest.mark.asyncio
async def test_async_function():
    pass

@pytest.mark.slow
def test_long_running():
    pass
```

---

## Writing Tests

### Unit Tests

Test individual functions in isolation:

```python
# services/rf-acquisition/tests/unit/test_iq_processor.py

import pytest
import numpy as np
from src.processors.iq_processor import compute_snr

def test_compute_snr_with_signal():
    """Test SNR computation with known signal."""
    # Generate synthetic data
    signal = np.sin(2 * np.pi * np.arange(1000) / 100)
    noise = 0.1 * np.random.randn(1000)
    iq_data = (signal + noise).astype(np.complex64)
    
    # Compute SNR
    snr = compute_snr(iq_data)
    
    # Assert reasonable range
    assert 10 < snr < 30, f"SNR {snr} out of expected range"
```

### Integration Tests

Test service communication with infrastructure:

```python
# services/tests/test_integration_services.py

import pytest
from services.tests.test_integration_base import IntegrationTestBase

class TestServiceIntegration(IntegrationTestBase):
    
    @pytest.mark.integration
    def test_redis_connectivity(self, redis_client):
        """Test Redis connection and operations."""
        redis_client.set('test_key', 'test_value')
        value = redis_client.get('test_key')
        assert value == 'test_value'
```

### E2E Tests

Test complete workflows:

```python
# services/tests/test_e2e_acquisition.py

import pytest
from services.rf_acquisition.tests.mock_websdrs import create_mock_fetcher

@pytest.mark.e2e
@pytest.mark.asyncio
async def test_e2e_acquire_and_store():
    """Test complete acquisition workflow."""
    mock_fetcher = create_mock_fetcher(receiver_count=7)
    
    result = await mock_fetcher.fetch_iq_simultaneous(
        frequency_mhz=145.50,
        duration_seconds=2.0
    )
    
    assert len(result) == 7
```

---

## Fixtures and Mocking

### Shared Fixtures

Located in `services/common/test_fixtures.py`:

```python
def test_with_database(test_db_session):
    """Test using in-memory database."""
    # test_db_session is automatically provided
    pass

def test_with_redis(mock_redis):
    """Test using Redis."""
    mock_redis.set('key', 'value')
    pass

def test_with_config(test_config):
    """Test using test configuration."""
    assert test_config['LOG_LEVEL'] == 'DEBUG'
```

### Mock WebSDRs

Eliminate network dependencies:

```python
from services.rf_acquisition.tests.mock_websdrs import (
    create_mock_fetcher,
    create_mock_websdrs,
    MockWebSDRReceiver
)

# Create mock receivers
receivers = create_mock_websdrs(count=7)

# Create fetcher with failures
mock_fetcher = create_mock_fetcher(
    receiver_count=7,
    failure_indices=[2, 4]  # Receivers 2 and 4 will fail
)

# Use in tests
result = await mock_fetcher.fetch_iq_simultaneous(145.50, 2.0)
```

### Custom Fixtures

Create service-specific fixtures in `conftest.py`:

```python
# services/rf-acquisition/tests/conftest.py

import pytest

@pytest.fixture
def sample_iq_data():
    """Generate sample IQ data."""
    import numpy as np
    return np.random.randn(1000).astype(np.complex64)
```

---

## Running Tests

### Local Testing

```bash
# Run all tests
make test-local

# Run unit tests only
make test-unit

# Run integration tests only
make test-integration

# Run E2E tests only
make test-e2e

# Run with coverage
make test-coverage

# Re-run failed tests
make test-failed

# Watch mode (auto-rerun on changes)
make test-watch
```

### Pytest Commands

```bash
# Run specific test file
pytest services/rf-acquisition/tests/unit/test_iq_processor.py

# Run specific test
pytest services/rf-acquisition/tests/unit/test_iq_processor.py::test_compute_snr

# Run tests matching pattern
pytest -k "test_snr"

# Run with verbose output
pytest -v

# Run with coverage
pytest --cov=services/rf-acquisition/src

# Skip slow tests
pytest -m "not slow"
```

### Docker-Based Testing

```bash
# Run tests in Docker containers
make test

# Test specific service
docker compose exec rf-acquisition pytest tests/
```

---

## Coverage Requirements

### Measuring Coverage

```bash
# Generate coverage report
make coverage-report

# View HTML report
open htmlcov/index.html

# Check coverage threshold
coverage report --fail-under=80
```

### Coverage Report

The `analyze_coverage.py` script generates:

- Terminal summary
- JSON report (`coverage-report.json`)
- HTML reports per service (`htmlcov/`)

### Coverage Thresholds

- **Fail CI**: < 80% average coverage
- **Warning**: 60-80% coverage
- **Pass**: ≥ 80% coverage

---

## CI/CD Integration

### GitHub Actions Workflow

The `test-coverage.yml` workflow runs on:

- Push to `main` or `develop`
- Pull requests
- Manual trigger

**Workflow Steps**:

1. Setup infrastructure (PostgreSQL, Redis, RabbitMQ)
2. Run unit tests
3. Run integration tests
4. Generate coverage reports
5. Upload to Codecov
6. Comment on PR with coverage

### Coverage Comments

PRs automatically receive coverage comments showing:

- Overall coverage percentage
- Coverage change vs. base branch
- Per-file coverage breakdown

---

## Troubleshooting

### Common Issues

#### 1. Import Errors

**Problem**: `ModuleNotFoundError` when running tests

**Solution**: Ensure `src/` is in Python path via `conftest.py`:

```python
# services/rf-acquisition/tests/conftest.py
import sys
from pathlib import Path

def pytest_configure(config):
    service_root = Path(__file__).parent.parent
    sys.path.insert(0, str(service_root / "src"))
```

#### 2. Async Test Failures

**Problem**: `RuntimeError: Event loop is closed`

**Solution**: Use `asyncio_mode = auto` in `pytest.ini`:

```ini
[pytest]
asyncio_mode = auto
```

#### 3. Docker Containers Not Running

**Problem**: Integration tests skip with "Container not found"

**Solution**: Start infrastructure:

```bash
make dev-up
```

#### 4. Flaky Tests

**Problem**: Tests pass/fail inconsistently

**Solution**: Use mocks instead of real network calls:

```python
from services.rf_acquisition.tests.mock_websdrs import create_mock_fetcher
```

#### 5. Coverage Not Increasing

**Problem**: Adding tests doesn't increase coverage

**Solution**: Ensure tests actually execute the code:

```bash
# Run with coverage to see what's missing
pytest --cov=src --cov-report=term-missing
```

### Debug Mode

Enable verbose output:

```bash
# Verbose pytest
pytest -vv -s

# Show print statements
pytest -s

# Show locals on failure
pytest -l

# Enter debugger on failure
pytest --pdb
```

---

## Best Practices

### 1. Test Naming

Use descriptive names:

```python
# Good
def test_compute_snr_with_zero_noise_returns_infinity():
    pass

# Bad
def test_snr():
    pass
```

### 2. Arrange-Act-Assert

Structure tests clearly:

```python
def test_something():
    # Arrange: Setup test data
    data = create_test_data()
    
    # Act: Execute the function
    result = process_data(data)
    
    # Assert: Verify the result
    assert result == expected_value
```

### 3. Use Fixtures

Avoid code duplication:

```python
@pytest.fixture
def sample_data():
    return {"key": "value"}

def test_with_fixture(sample_data):
    assert sample_data["key"] == "value"
```

### 4. Test Edge Cases

```python
def test_process_empty_array():
    result = process(np.array([]))
    assert len(result) == 0

def test_process_single_element():
    result = process(np.array([1]))
    assert result[0] == expected

def test_process_large_array():
    large_array = np.random.randn(1_000_000)
    result = process(large_array)
    assert len(result) == len(large_array)
```

### 5. Mock External Dependencies

```python
@patch('requests.get')
def test_api_call(mock_get):
    mock_get.return_value.json.return_value = {'data': 'test'}
    result = fetch_data()
    assert result == {'data': 'test'}
```

---

## Resources

- [Pytest Documentation](https://docs.pytest.org/)
- [pytest-asyncio](https://pytest-asyncio.readthedocs.io/)
- [Coverage.py](https://coverage.readthedocs.io/)
- [Heimdall Architecture Docs](../ARCHITECTURE.md)

---

## Contact

Questions? Issues?

- Open an issue on GitHub
- Email: alessio.corsi@gmail.com
- See [CONTRIBUTING.md](../../CONTRIBUTING.md)
